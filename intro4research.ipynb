{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification example: cats vs dogs\n",
    "\n",
    "Learn how to use Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about:\n",
    "\n",
    "- Tensors, variables, and gradients in TensorFlow\n",
    "- Creating layers by subclassing the Layer class\n",
    "- Writing low-level training loops\n",
    "- Tracking losses created by layers via the add_loss() method\n",
    "- Tracking metrics in a low-level training loop\n",
    "- Speeding up execution with a compiled tf.function\n",
    "- Executing layers in training or inference mode\n",
    "- The Keras Functional API\n",
    "\n",
    "## Tensors\n",
    "\n",
    "TensorFlow is an infrastructure layer for differentiable programming. At its heart, it's a framework for manipulating N-dimensional arrays (tensors), much like NumPy.\n",
    "\n",
    "However, there are three key differences between NumPy and TensorFlow:\n",
    "\n",
    "1. TensorFlow can leverage hardware accelerators such as GPUs and TPUs.\n",
    "2. TensorFlow can automatically compute the gradient of arbitrary differentiable tensor expressions.\n",
    "3. TensorFlow computation can be distributed to large numbers of devices on a single machine, and large number of machines (potentially with multiple devices each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_tensor = tf.constant([[1, 0], [0, 1]])\n",
    "constant_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the tensor's values as a Numpy nd-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find its type and shape the same way of Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'DType: {constant_tensor.dtype}')\n",
    "print(f'Shape: {constant_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant tensors of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.ones(shape=(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.zeros(shape=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random uniform tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random from normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.normal(shape=(1, 3), mean=0., stddev=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables are special tensors used to store mutable state (such as the weights of a neural network). You create a variable using some initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_tensor = tf.random.normal(shape=(1, 10))\n",
    "variable = tf.Variable(gaussian_tensor)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update a variable just use the <code>sub/add/assign</code> methods. Keep in mind that like arrays and matrixes the shape must be coherent to allow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign new tensor to variable\n",
    "new_gaussian_tensor = tf.random.normal(shape=(1, 10))\n",
    "variable.assign(new_gaussian_tensor)\n",
    "# verify the assign operation is true\n",
    "for i in range(new_gaussian_tensor.shape[0]):\n",
    "    for j in range(new_gaussian_tensor.shape[1]):\n",
    "        assert new_gaussian_tensor[i, j] == new_gaussian_tensor[i, j]\n",
    "\n",
    "# add tensor to variable\n",
    "added_uniform_tensor = tf.random.uniform(shape=(1, 10))\n",
    "variable.assign_add(added_uniform_tensor)\n",
    "# verify the assign + add operation is true\n",
    "for i in range(new_gaussian_tensor.shape[0]):\n",
    "    for j in range(new_gaussian_tensor.shape[1]):\n",
    "        assert variable[i, j] == new_gaussian_tensor[i, j] + added_uniform_tensor[i, j]\n",
    "\n",
    "# subtract tensor to variable\n",
    "subbed_uniform_tensor = tf.random.uniform(shape=(1, 10))\n",
    "variable.assign_sub(subbed_uniform_tensor)\n",
    "# verify the assign + add + sub operation is true\n",
    "for i in range(new_gaussian_tensor.shape[0]):\n",
    "    for j in range(new_gaussian_tensor.shape[1]):\n",
    "        assert variable[i, j] == new_gaussian_tensor[i, j] + added_uniform_tensor[i, j] - subbed_uniform_tensor[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing math in TensorFlow\n",
    "\n",
    "If you've used NumPy, doing math in TensorFlow will look very familiar. The main difference is that your TensorFlow code can run on GPU and TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "# sum\n",
    "c = a + b\n",
    "# subtraction\n",
    "d = a - b\n",
    "# moltiplication\n",
    "e = a * b\n",
    "# division\n",
    "f = a / b\n",
    "# square\n",
    "g = tf.square(c)\n",
    "# exponential\n",
    "h = tf.exp(c)\n",
    "\n",
    "c, d, e, f, g, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Here's another big difference with NumPy: you can automatically retrieve the gradient of any differentiable expression. Just open a GradientTape, start \"watching\" a tensor via <code>watch</code> method, and compose a differentiable expression using this tensor as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # start recording the history of operations applied to a\n",
    "    tape.watch(a)  \n",
    "    # do operations\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  \n",
    "    # what's the gradient of c with respect to a?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, <b>variables are watched automatically</b>, so you don't need to manually watch them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(a)\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can compute higher-order derivatives by nesting tapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers\n",
    "\n",
    "While TensorFlow is an infrastructure layer for differentiable programming, dealing with tensors, variables, and gradients. Keras is a user interface for deep learning, dealing with layers, models, optimizers, loss functions, metrics, and more. Keras serves as the high-level API for TensorFlow: Keras is what makes TensorFlow simple and productive.\n",
    "\n",
    "The <code>Layer</code> class is the fundamental abstraction in Keras. A layer encapsulates a state (weights) and some computation (defined in the call method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"Linear layer of the type:\n",
    "        y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"), trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would use a Layer instance much like a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate layer\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# the layer can be called as a function\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert the shape of the layer\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight variables (created in <code>__init__</code>) are automatically tracked under the weights property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have many built-in layers available, from Dense to Conv2D to LSTM to fancier ones like Conv3DTranspose or ConvLSTM2D. Be smart about reusing built-in functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer weight creation\n",
    "\n",
    "The <code>self.add_weight()</code> method gives you a shortcut for creating weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"Linear layer of the type:\n",
    "        y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "linear_layer = Linear(units=4)\n",
    "\n",
    "# this will also call \"build\" and create weights\n",
    "y = linear_layer(tf.ones(shape=(2, 2)))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer gradients\n",
    "\n",
    "You can automatically retrieve the gradients of the weights of a layer by calling it inside a GradientTape. Using these gradients, you can update the weights of the layer, either manually, or using an optimizer object. Of course, you can modify the gradients before using them, if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensors=(x_train.reshape(60000, 784).astype(\"float32\")/255, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# instantiate our linear layer (defined above) with 10 units\n",
    "linear_layer = Linear(10)\n",
    "# instantiate a logistic loss function that expects integer targets\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# iterate over the batches of the dataset\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    # open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward pass\n",
    "        logits = linear_layer(x)\n",
    "        # loss value for this batch\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # get gradients of the loss wrt the weights\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "    # update the weights of our linear layer\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # logging\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable and non-trainable weights\n",
    "\n",
    "Weights created by layers can be either trainable or non-trainable. They're exposed in trainable_weights and non_trainable_weights respectively. Here's a layer with a non-trainable weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # create a non-trainable weight\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert weights\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers that own layers\n",
    "\n",
    "Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reuse the Linear class with a `build` method that we defined above.\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    # stacking of the 3 layers\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "mlp = MLP()\n",
    "# the first call to the `mlp` object will create the weights\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights are recursively tracked\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our manually-created MLP above is equivalent to the following built-in option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential([keras.layers.Dense(32, activation=tf.nn.relu), keras.layers.Dense(32, activation=tf.nn.relu), keras.layers.Dense(10)])\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
