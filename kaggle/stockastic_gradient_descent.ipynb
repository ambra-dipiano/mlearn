{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.\n",
    "\n",
    "As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal's 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\n",
    "\n",
    "In addition to the training data, we need two more things:\n",
    "\n",
    "A \"loss function\" that measures how good the network's predictions are.\n",
    "An \"optimizer\" that can tell the network how to change its weights.\n",
    "The Loss Function\n",
    "We've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\n",
    "\n",
    "The loss function measures the disparity between the the target's true value and the value the model predicts.\n",
    "\n",
    "Different problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\n",
    "\n",
    "A common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
    "\n",
    "The total MAE loss on a dataset is the mean of all these absolute differences.\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/VDcvkZN.png\" width=\"300\">\n",
    "\n",
    "<i>The mean absolute error is the average length between the fitted curve and the data points.</i></center>\n",
    "\n",
    "Besides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\n",
    "\n",
    "During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\n",
    "\n",
    "# The Optimizer - Stochastic Gradient Descent\n",
    "\n",
    "We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
    "\n",
    "Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "\n",
    "Sample some training data and run it through the network to make predictions.\n",
    "Measure the loss between the predictions and the true values.\n",
    "Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"500\">\n",
    "\n",
    "<i>Training a neural network with Stochastic Gradient Descent.</i></center>\n",
    "\n",
    "Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n",
    "\n",
    "The animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\n",
    "\n",
    "# Learning Rate and Batch Size\n",
    "Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
    "\n",
    "The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\n",
    "\n",
    "Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\n",
    "\n",
    "# Adding the Loss and Optimizer\n",
    "After defining a model, you can add a loss function and optimizer with the model's compile method:\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "```\n",
    "\n",
    "Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.\n",
    "\n",
    "### What's In a Name?\n",
    "\n",
    "The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!\n",
    "\n",
    "## Example - Red Wine Quality\n",
    "\n",
    "Now we know everything we need to start training deep learning models. So let's see it in action! We'll use the Red Wine Quality dataset.\n",
    "\n",
    "This dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests. How well can we predict a wine's perceived quality from these measurements?\n",
    "\n",
    "We've put all of the data preparation into this next hidden cell. It's not essential to what follows so feel free to skip it. One thing you might note for now though is that we've rescaled each feature to lie in the interval  [0,1] . As we'll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>LongestShell</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>M</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>I</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>M</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2555</td>\n",
       "      <td>0.5270</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>I</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  LongestShell  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "668     M          0.55     0.425   0.155       0.9175         0.2775   \n",
       "1580    I          0.50     0.400   0.120       0.6160         0.2610   \n",
       "3784    M          0.62     0.480   0.155       1.2555         0.5270   \n",
       "463     I          0.22     0.165   0.055       0.0545         0.0215   \n",
       "\n",
       "      VisceraWeight  ShellWeight  Rings  \n",
       "668           0.243       0.3350     13  \n",
       "1580          0.143       0.1935      8  \n",
       "3784          0.374       0.3175     11  \n",
       "463           0.012       0.0200      5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:163\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/computation/expressions.py:239\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/computation/expressions.py:69\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     68\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m max_ \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m min_ \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m df_train \u001b[38;5;241m=\u001b[39m (\u001b[43mdf_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_\u001b[49m) \u001b[38;5;241m/\u001b[39m (max_ \u001b[38;5;241m-\u001b[39m min_)\n\u001b[1;32m     16\u001b[0m df_valid \u001b[38;5;241m=\u001b[39m (df_valid \u001b[38;5;241m-\u001b[39m min_) \u001b[38;5;241m/\u001b[39m (max_ \u001b[38;5;241m-\u001b[39m min_)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Split features and target\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/ops/common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/arraylike.py:108\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/frame.py:6940\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6936\u001b[0m other \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmaybe_prepare_scalar_for_op(other, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis],))\n\u001b[1;32m   6938\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39malign_method_FRAME(\u001b[38;5;28mself\u001b[39m, other, axis, flex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 6940\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_frame_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/frame.py:6979\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[0;34m(self, right, func, axis)\u001b[0m\n\u001b[1;32m   6973\u001b[0m     \u001b[38;5;66;03m# TODO: The previous assertion `assert right._indexed_same(self)`\u001b[39;00m\n\u001b[1;32m   6974\u001b[0m     \u001b[38;5;66;03m#  fails in cases with empty columns reached via\u001b[39;00m\n\u001b[1;32m   6975\u001b[0m     \u001b[38;5;66;03m#  _frame_arith_method_with_reindex\u001b[39;00m\n\u001b[1;32m   6976\u001b[0m \n\u001b[1;32m   6977\u001b[0m     \u001b[38;5;66;03m# TODO operate_blockwise expects a manager of the same type\u001b[39;00m\n\u001b[1;32m   6978\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6979\u001b[0m         bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperate_blockwise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6980\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# error: Argument 1 to \"operate_blockwise\" of \"ArrayManager\" has\u001b[39;49;00m\n\u001b[1;32m   6981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[1;32m   6982\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"ArrayManager\"\u001b[39;49;00m\n\u001b[1;32m   6983\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# error: Argument 1 to \"operate_blockwise\" of \"BlockManager\" has\u001b[39;49;00m\n\u001b[1;32m   6984\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# incompatible type \"Union[ArrayManager, BlockManager]\"; expected\u001b[39;49;00m\n\u001b[1;32m   6985\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"BlockManager\"\u001b[39;49;00m\n\u001b[1;32m   6986\u001b[0m \u001b[43m            \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   6987\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6988\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(bm)\n\u001b[1;32m   6991\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, Series) \u001b[38;5;129;01mand\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   6992\u001b[0m     \u001b[38;5;66;03m# axis=1 means we want to operate row-by-row\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/internals/managers.py:1409\u001b[0m, in \u001b[0;36mBlockManager.operate_blockwise\u001b[0;34m(self, other, array_op)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moperate_blockwise\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: BlockManager, array_op) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BlockManager:\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;124;03m    Apply array_op blockwise with another (aligned) BlockManager.\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperate_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/internals/ops.py:63\u001b[0m, in \u001b[0;36moperate_blockwise\u001b[0;34m(left, right, array_op)\u001b[0m\n\u001b[1;32m     61\u001b[0m res_blks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lvals, rvals, locs, left_ea, right_ea, rblk \u001b[38;5;129;01min\u001b[39;00m _iter_block_pairs(left, right):\n\u001b[0;32m---> 63\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43marray_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_ea \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_ea \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(res_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     65\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:222\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[0;32m--> 222\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:170\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (is_object_dtype(left\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(right)):\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;66;03m#  on the non-missing values)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mlearn/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:108\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 108\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "red_wine = pd.read_csv('dataset/wine/abalone.csv')\n",
    "red_wine.dropna()\n",
    "\n",
    "# create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index)\n",
    "display(df_train.head(4))\n",
    "\n",
    "# scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_train.drop('quality', axis=1)\n",
    "X_valid = df_valid.drop('quality', axis=1)\n",
    "y_train = df_train['quality']\n",
    "y_valid = df_valid['quality']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49102747a477ec872ce5c57af32e04f52205abcec8f436ba78cc541d2e49f433"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
